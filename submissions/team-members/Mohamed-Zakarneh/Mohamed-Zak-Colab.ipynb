{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Packages"
      ],
      "metadata": {
        "id": "fawJ1g-U4N3p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49Ti2u_j39mZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization, Add, ReLU, GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from time import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "import math\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from skimage.measure import shannon_entropy\n",
        "import albumentations as A\n",
        "from albumentations.augmentations.dropout.coarse_dropout import CoarseDropout"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "UPzajcci4SgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Performance\n",
        "\n",
        "def model_performance(df):\n",
        "  print('--------Training-----------')\n",
        "  print(f'Claasification Accuravy: {max(df[\"accuracy\"])}')\n",
        "  print(f'Claasification Loss: {min(df[\"loss\"])}')\n",
        "  print(f'Classfication Recall: {max(df[\"recall\"])}')\n",
        "  print(f'Classfication precision: {max(df[\"precision\"])}')\n",
        "  print(f'Classfication AUC: {max(df[\"auc\"])}')\n",
        "  print('--------Validation-----------')\n",
        "  print(f'Val Claasification Accuravy: {max(df[\"val_accuracy\"])}')\n",
        "  print(f'Val Claasification Loss: {min(df[\"val_loss\"])}')\n",
        "  print(f'Val Classfication Recall: {max(df[\"val_recall\"])}')\n",
        "  print(f'Val Classfication precision: {max(df[\"val_precision\"])}')\n",
        "  print(f'Val Classfication AUC: {max(df[\"val_auc\"])}')\n",
        "  print('--------Learning Rate-----------')\n",
        "  print(f'Min Learning Rate: {min(df[\"learning_rate\"])}')\n",
        "  print(f'Max Learning Rate: {max(df[\"learning_rate\"])}')\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(df[\"loss\"], label=\"Train Loss\")\n",
        "  plt.plot(df[\"val_loss\"], label=\"Validation Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(\"Loss vs. Epochs\")\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "  # Plot Accuracy vs. Epochs\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(df[\"accuracy\"], label=\"Train Accuracy\")\n",
        "  plt.plot(df[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.title(\"Accuracy vs. Epochs\")\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot Accuracy vs. Epochs\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(df[\"recall\"], label=\"Train recall\")\n",
        "  plt.plot(df[\"val_recall\"], label=\"Validation recall\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"recall\")\n",
        "  plt.title(\"recall vs. Epochs\")\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot Accuracy vs. Epochs\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(df[\"precision\"], label=\"Train precision\")\n",
        "  plt.plot(df[\"val_precision\"], label=\"Validation precision\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"precision\")\n",
        "  plt.title(\"precision vs. Epochs\")\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "ZewZNTsw4UWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## One Image Test\n",
        "def one_image_test(model, data_set, class_ids = ['H1', 'H2', 'H3', 'H5', 'H6']):\n",
        "  i = np.random.randint(0, len(data_set))\n",
        "  actual_class = data_set.iloc[i].class_id\n",
        "  img_path = data_set.iloc[i].img_path\n",
        "  img_name = data_set.iloc[i].img_name\n",
        "\n",
        "  try:\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_LANCZOS4)\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    img_input = np.expand_dims(img, axis=0)\n",
        "    result = model.predict(img_input)\n",
        "    predicted_class = class_ids[result.argmax()]\n",
        "    confidence = result.max()\n",
        "    print(f'Image Name: {img_name}, Actual Class: {actual_class}, Predicted Class: {predicted_class}, Confidence: {confidence}')\n",
        "\n",
        "  except:\n",
        "    print(data_set.iloc[i].img_path)"
      ],
      "metadata": {
        "id": "qZmCq--o4YGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GradCAM\n",
        "def grad_cam(model, image, layer_name):\n",
        "    # Ensure the image has the correct shape\n",
        "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
        "\n",
        "    # Get the model's expected input structure\n",
        "    if isinstance(model.input, list):\n",
        "        inputs = [image]  # Wrap in a list if needed\n",
        "    else:\n",
        "        inputs = image  # Pass as a single tensor\n",
        "\n",
        "    # Get the output of the last convolutional layer\n",
        "    conv_layer = model.get_layer(layer_name)\n",
        "    grad_model = Model(inputs=model.inputs, outputs=[conv_layer.output, model.output])\n",
        "\n",
        "    # Compute gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(inputs)\n",
        "        loss = predictions[:, np.argmax(predictions[0])]  # Use the predicted class\n",
        "\n",
        "    grads = tape.gradient(loss, conv_outputs)\n",
        "    if grads is None:\n",
        "        raise ValueError(f\"Gradients are None for layer {layer_name}. Check if the layer is trainable.\")\n",
        "\n",
        "    grads = grads[0]\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # Compute the heatmap\n",
        "    conv_outputs = conv_outputs[0]  # Remove batch dimension\n",
        "    pooled_grads = pooled_grads[..., tf.newaxis]  # Add a new axis for matrix multiplication\n",
        "    heatmap = tf.reduce_sum(conv_outputs * pooled_grads, axis=-1)\n",
        "    heatmap = tf.maximum(heatmap, 0)  # ReLU\n",
        "\n",
        "    # Normalize the heatmap\n",
        "    heatmap_max = tf.reduce_max(heatmap)\n",
        "    if heatmap_max == 0:  # Avoid division by zero\n",
        "        print(f\"Warning: Heatmap for layer {layer_name} is all zeros.\")\n",
        "        heatmap = tf.zeros_like(heatmap)  # Return a zero heatmap\n",
        "    else:\n",
        "        heatmap /= heatmap_max\n",
        "\n",
        "    # Resize heatmap to match the input image size\n",
        "    heatmap = cv2.resize(heatmap.numpy(), (image.shape[2], image.shape[1]))\n",
        "    return heatmap"
      ],
      "metadata": {
        "id": "9HuZmXSG4aX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"âœ… GPU successfully assigned!\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "rV2mXHwA5Paz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set = pd.read_csv('/content/drive/MyDrive/SDS-CP-31/data_set.csv', index_col=0)"
      ],
      "metadata": {
        "id": "6Mca2nk95SAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "26ty_LUt5V0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edge Density Analsys"
      ],
      "metadata": {
        "id": "fFyhAPP_5YvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_sampled = data_set.groupby('class_id').sample(n=100, random_state=40)"
      ],
      "metadata": {
        "id": "FlEfZlWj5Urt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_results = pd.DataFrame(columns=['class_id', 'mean_edge', 'edge_density', 'entropy'])"
      ],
      "metadata": {
        "id": "dl_1DyH95lec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "threshold = 30\n",
        "for i in range (data_sampled.shape[0]):\n",
        "  actual_class = data_sampled.iloc[i].class_id\n",
        "  img_path = data_sampled.iloc[i].img_path\n",
        "  try:\n",
        "    img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "  sobel_x = cv2.Sobel(img_gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "  sobel_y = cv2.Sobel(img_gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "  edge_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)\n",
        "  mean_edge = edge_magnitude.mean()\n",
        "  density = np.sum(edge_magnitude > threshold) / edge_magnitude.size\n",
        "  edge_density = np.sum(edge_magnitude > threshold) / edge_magnitude.size\n",
        "  entropy = shannon_entropy(edge_magnitude)\n",
        "\n",
        "  results.append([actual_class, mean_edge, edge_density, entropy])\n",
        "\n",
        "edge_results = pd.DataFrame(results, columns=['class_id', 'mean_edge', 'edge_density', 'entropy'])"
      ],
      "metadata": {
        "id": "KlRoj9mp5n6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_results.groupby('class_id').mean()"
      ],
      "metadata": {
        "id": "75EmMfpr5raL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "kzkOxOfA5vFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = Input(shape=(224, 224, 3))\n",
        "\n",
        "# Initial Conv Layer\n",
        "x = layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(inputs)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ReLU()(x)\n",
        "\n",
        "# Conv Block 2\n",
        "x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = ReLU()(x)\n",
        "\n",
        "x = MaxPooling2D((3, 3))(x)\n",
        "x = Dropout(0.1)(x)\n",
        "\n",
        "# Residual Block 1\n",
        "shortcut1 = x\n",
        "x = layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ReLU()(x)\n",
        "x = layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Add()([x, shortcut1])\n",
        "x = layers.ReLU()(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "# Residual Block 2\n",
        "shortcut2 = layers.Conv2D(128, (1, 1), padding='same')(x)\n",
        "x = layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ReLU()(x)\n",
        "x = layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Add()([x, shortcut2])\n",
        "x = layers.ReLU()(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "# Residual Block 3\n",
        "shortcut3 = layers.Conv2D(256, (1, 1), padding='same')(x)\n",
        "x = layers.Conv2D(256, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ReLU()(x)\n",
        "x = layers.Conv2D(256, (3, 3), padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Add()([x, shortcut3])\n",
        "x = layers.ReLU()(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "# Classification Head\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-3))(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(5, activation='softmax')(x)\n",
        "\n",
        "model4 = Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "-A4acHPq5x9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "yIlEW1Fn514M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val = train_test_split(\n",
        "    data_set,\n",
        "    test_size=0.2,\n",
        "    stratify=data_set['class_id'],\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "EQWZhezY54k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def balanced_batch_generator(df, batch_size=20):\n",
        "    class_ids = ['H1', 'H2', 'H3', 'H5', 'H6']\n",
        "    class_to_idx = {cls: i for i, cls in enumerate(class_ids)}\n",
        "    samples_per_class = batch_size // len(class_ids)\n",
        "\n",
        "    while True:\n",
        "        batch_df = pd.concat([\n",
        "            df[df['class_id'] == cls].sample(samples_per_class, replace=True)\n",
        "            for cls in class_ids\n",
        "        ]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "        images, labels = [], []\n",
        "\n",
        "        for _, row in batch_df.iterrows():\n",
        "            img_path = row['img_path']\n",
        "            label_str = row['class_id']\n",
        "            label_idx = class_to_idx[label_str]\n",
        "\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_LANCZOS4)\n",
        "            img = img.astype(np.float32) / 255.0\n",
        "\n",
        "            if img.ndim == 2:\n",
        "                img = np.stack([img]*3, axis=-1)\n",
        "            elif img.shape[2] == 1:\n",
        "                img = np.concatenate([img]*3, axis=-1)\n",
        "\n",
        "            images.append(img)\n",
        "            labels.append(tf.keras.utils.to_categorical(label_idx, num_classes=5))\n",
        "\n",
        "            if len(images) >= batch_size:\n",
        "                break\n",
        "\n",
        "        yield np.array(images), np.array(labels)"
      ],
      "metadata": {
        "id": "jBW_xU_C6Bhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = balanced_batch_generator(df_train, apply_aug=False, batch_size=20)\n",
        "val_gen = balanced_batch_generator(df_val, apply_aug=False, batch_size=20)"
      ],
      "metadata": {
        "id": "cK0Nv5Ke54n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batchsize = 20\n",
        "steps_per_epoch = max(1, len(df_train) // batchsize)\n",
        "validation_steps = max(1, len(df_val) // batchsize)\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss', factor=0.2, patience=4, min_lr=1e-6)"
      ],
      "metadata": {
        "id": "l2i83tVr54qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time()\n",
        "history = model4.fit(\n",
        "        train_gen,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        epochs=30,\n",
        "        validation_data=val_gen,\n",
        "        validation_steps=validation_steps,\n",
        "        batch_size=batchsize,\n",
        "        callbacks=lr_scheduler,\n",
        "        verbose=1\n",
        "    )\n",
        "end = time()\n",
        "print(f'Time Taken: {(end-start)/60} min')"
      ],
      "metadata": {
        "id": "fJjaS7hs54tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history41_df = pd.DataFrame(history.history)"
      ],
      "metadata": {
        "id": "nwO1kapS6cHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Testing"
      ],
      "metadata": {
        "id": "Jp_DVZks6im9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_performance(history41_df)"
      ],
      "metadata": {
        "id": "R5zIJKMO54wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_sampled = data_set.groupby('class_id').sample(n=100, random_state=40)"
      ],
      "metadata": {
        "id": "MWoU_JVt54zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_res_500 = pd.DataFrame(columns=['actual_claass', 'predicted_class', 'confidence'])"
      ],
      "metadata": {
        "id": "EdOzKDYJ5415"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_ids = ['H1', 'H2', 'H3', 'H5', 'H6']\n",
        "for i in range (data_sampled.shape[0]):\n",
        "  actual_class = data_sampled.iloc[i].class_id\n",
        "  img_path = data_sampled.iloc[i].img_path\n",
        "\n",
        "  try:\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_LANCZOS4)\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    img_input = np.expand_dims(img, axis=0)\n",
        "    result = model4.predict(img_input)\n",
        "    predicted_class = class_ids[result.argmax()]\n",
        "    confidence = result.max()\n",
        "    test_res_500.loc[i] = [actual_class, predicted_class, confidence]\n",
        "  except:\n",
        "    print(data_sampled.iloc[i].img_path)"
      ],
      "metadata": {
        "id": "NjSescXo544r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(test_res_500['actual_claass'], test_res_500['predicted_class'], labels=sorted(test_res_500['actual_claass'].unique()))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=sorted(test_res_500['actual_claass'].unique()),\n",
        "            yticklabels=sorted(test_res_500['actual_claass'].unique()))\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pJFcD5Cn6trx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GradCAM"
      ],
      "metadata": {
        "id": "6EnG2GZS6xGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model4_layer_names = ['conv2d', 'conv2d_1', 'conv2d_2', 'conv2d_3', 'conv2d_4', 'conv2d_5', 'conv2d_8', 'conv2d_9', 'conv2d_7']"
      ],
      "metadata": {
        "id": "SkKck0Fe62Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = np.random.randint(0, len(df_val))\n",
        "test_img_path = df_val.iloc[i]['img_path']\n",
        "img = cv2.imread(test_img_path)\n",
        "img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_LANCZOS4)\n",
        "cv2_imshow(img)\n",
        "\n",
        "img = img / 255.0  # Normalize\n",
        "for j in model4_layer_names:\n",
        "    try:\n",
        "        heatmap = grad_cam(model4, img, j)  # Tumor class\n",
        "        if np.max(heatmap) > 0:  # Only plot if heatmap is not blank\n",
        "            plt.imshow(heatmap, cmap=\"jet\")\n",
        "            plt.title(f\"Grad-CAM for {j}\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Skipping layer {j} because the heatmap is blank.\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Error for layer {j}: {e}\")"
      ],
      "metadata": {
        "id": "N2DKZyNL621c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}