{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow torchinfo torch torchvision torchsummary opencv-python tqdm Pillow scikit-learn"
      ],
      "metadata": {
        "id": "Ia1EyCxZpHCs"
      },
      "id": "Ia1EyCxZpHCs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "570f8247",
      "metadata": {
        "papermill": {
          "duration": 0.008701,
          "end_time": "2023-11-08T14:59:13.941158",
          "exception": false,
          "start_time": "2023-11-08T14:59:13.932457",
          "status": "completed"
        },
        "tags": [],
        "id": "570f8247"
      },
      "source": [
        "## Import lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c502bea7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-08T14:59:13.956577Z",
          "iopub.status.busy": "2023-11-08T14:59:13.955820Z",
          "iopub.status.idle": "2023-11-08T14:59:32.812626Z",
          "shell.execute_reply": "2023-11-08T14:59:32.811733Z"
        },
        "papermill": {
          "duration": 18.867512,
          "end_time": "2023-11-08T14:59:32.815006",
          "exception": false,
          "start_time": "2023-11-08T14:59:13.947494",
          "status": "completed"
        },
        "tags": [],
        "id": "c502bea7"
      },
      "outputs": [],
      "source": [
        "# Basic data manipulations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Handling images\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Handling paths\n",
        "\n",
        "import time\n",
        "\n",
        "# Pytorch essentials\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import models\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchsummary\n",
        "\n",
        "\n",
        "\n",
        "# Pytorch essentials for datasets.\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Pytorch way of data augmentation.\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms, utils\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#PyTorch Monitoring to MLflow\n",
        "import mlflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accessing the Dataset from Google Drive"
      ],
      "metadata": {
        "id": "cCxd8h6WR3zi"
      },
      "id": "cCxd8h6WR3zi"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9JFk7BdLRm_Z"
      },
      "id": "9JFk7BdLRm_Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = '/content/drive/MyDrive/SDS-myconet/DeFungi/'\n",
        "#root_path = 'E:\\SDS\\SDS-CP031-myconet\\DeFungi'"
      ],
      "metadata": {
        "id": "m_fw6xzGSAiX"
      },
      "id": "m_fw6xzGSAiX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3d3ced71",
      "metadata": {
        "papermill": {
          "duration": 0.005424,
          "end_time": "2023-11-08T14:59:32.826193",
          "exception": false,
          "start_time": "2023-11-08T14:59:32.820769",
          "status": "completed"
        },
        "tags": [],
        "id": "3d3ced71"
      },
      "source": [
        "## Create dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea659f6f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-08T14:59:32.839280Z",
          "iopub.status.busy": "2023-11-08T14:59:32.838003Z",
          "iopub.status.idle": "2023-11-08T14:59:40.281135Z",
          "shell.execute_reply": "2023-11-08T14:59:40.280096Z"
        },
        "papermill": {
          "duration": 7.452142,
          "end_time": "2023-11-08T14:59:40.283650",
          "exception": false,
          "start_time": "2023-11-08T14:59:32.831508",
          "status": "completed"
        },
        "tags": [],
        "id": "ea659f6f"
      },
      "outputs": [],
      "source": [
        "#root_path = '/kaggle/input/defungi/'\n",
        "df = pd.DataFrame({\"path\":[],\"label\":[], \"class_id\":[]})\n",
        "label_dict = {\n",
        "    \"H1\":0,\n",
        "    \"H2\":1,\n",
        "    \"H3\":2,\n",
        "    \"H5\":3,\n",
        "    \"H6\":4,\n",
        "}\n",
        "for key in label_dict:\n",
        "    img_path = os.path.join(root_path, key)\n",
        "    jpg_path_list = glob(img_path+'/*.jpg')\n",
        "    for jpg_path in jpg_path_list:\n",
        "        new_data_frame =pd.DataFrame({\"path\":jpg_path,\"label\":key, \"class_id\":label_dict[key]}, index=[1])\n",
        "        df = pd.concat([df, new_data_frame], ignore_index=True)\n",
        "\n",
        "df[[\"path\"]] = df[[\"path\"]].astype(str)\n",
        "df[[\"label\"]] = df[[\"label\"]].astype(str)\n",
        "df[[\"class_id\"]] = df[[\"class_id\"]].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display DataFrame"
      ],
      "metadata": {
        "id": "YqlbZWWAaWyl"
      },
      "id": "YqlbZWWAaWyl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d518c65",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-08T14:59:40.296180Z",
          "iopub.status.busy": "2023-11-08T14:59:40.295826Z",
          "iopub.status.idle": "2023-11-08T14:59:40.309194Z",
          "shell.execute_reply": "2023-11-08T14:59:40.308274Z"
        },
        "papermill": {
          "duration": 0.021604,
          "end_time": "2023-11-08T14:59:40.311064",
          "exception": false,
          "start_time": "2023-11-08T14:59:40.289460",
          "status": "completed"
        },
        "tags": [],
        "id": "3d518c65"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Show Class Distribution\n",
        "\n",
        "**Class distribution in histogram**"
      ],
      "metadata": {
        "id": "3KNAfgcPabtq"
      },
      "id": "3KNAfgcPabtq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf387db2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-08T14:59:40.323499Z",
          "iopub.status.busy": "2023-11-08T14:59:40.323179Z",
          "iopub.status.idle": "2023-11-08T14:59:40.581612Z",
          "shell.execute_reply": "2023-11-08T14:59:40.580630Z"
        },
        "papermill": {
          "duration": 0.267153,
          "end_time": "2023-11-08T14:59:40.583767",
          "exception": false,
          "start_time": "2023-11-08T14:59:40.316614",
          "status": "completed"
        },
        "tags": [],
        "id": "bf387db2"
      },
      "outputs": [],
      "source": [
        "plt.hist(df['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Class Imbalance**\n",
        "\n",
        "Since our dataset is imbalanced and we want to use a weighted loss function, we will use the weight argument in PyTorch's loss functions like nn.CrossEntropyLoss. This allows us to assign different weights to the loss for each class, giving more importance to the minority classes.\n"
      ],
      "metadata": {
        "id": "cuTYXj4nYpRO"
      },
      "id": "cuTYXj4nYpRO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate class frequencies\n",
        "class_counts = df['class_id'].value_counts().sort_index()\n",
        "total_samples = len(df)\n",
        "class_weights = total_samples / (len(class_counts) * class_counts)\n",
        "\n",
        "# Convert to a PyTorch tensor and move to the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
        "class_weights_tensor = torch.tensor(class_weights.values, dtype=torch.float32).to(device)\n",
        "print(\"Class Weights:\", class_weights_tensor)"
      ],
      "metadata": {
        "id": "mq9Ks3RKZacz"
      },
      "id": "mq9Ks3RKZacz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display sample images from Dataset"
      ],
      "metadata": {
        "id": "g1S_D36-a7AM"
      },
      "id": "g1S_D36-a7AM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9054ecad",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-08T14:59:40.597255Z",
          "iopub.status.busy": "2023-11-08T14:59:40.596751Z",
          "iopub.status.idle": "2023-11-08T14:59:42.990144Z",
          "shell.execute_reply": "2023-11-08T14:59:42.989122Z"
        },
        "papermill": {
          "duration": 2.410807,
          "end_time": "2023-11-08T14:59:43.000670",
          "exception": false,
          "start_time": "2023-11-08T14:59:40.589863",
          "status": "completed"
        },
        "tags": [],
        "id": "9054ecad"
      },
      "outputs": [],
      "source": [
        "show_imgs = 15\n",
        "idx = np.random.randint(0, len(df),size=show_imgs)\n",
        "fig, axes = plt.subplots(show_imgs//5, 5, figsize=(15,10))\n",
        "axes = axes.flatten()\n",
        "for i, ax in enumerate(axes):\n",
        "    full_path = df.loc[idx[i]]['path']\n",
        "    ax.imshow(plt.imread(full_path))\n",
        "    ax.set_title(df.loc[idx[i]]['label'])\n",
        "    ax.set_axis_off()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e95efa7a",
      "metadata": {
        "papermill": {
          "duration": 0.015432,
          "end_time": "2023-11-08T14:59:43.032066",
          "exception": false,
          "start_time": "2023-11-08T14:59:43.016634",
          "status": "completed"
        },
        "tags": [],
        "id": "e95efa7a"
      },
      "source": [
        "## Create Image Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image Transformations Explained**\n",
        "The code defines three sets of transformations using torchvision.transforms.v2.Compose:\n",
        "\n",
        "train_transforms: These transformations are applied specifically to the images used for training the model. They include several data augmentation techniques to help the model generalize better and prevent overfitting.\n",
        "\n",
        "eval_transforms: These transformations are for the validation set, which is used during training to evaluate the model's performance on data it hasn't seen before. Data augmentation is typically not applied here, only transformations needed to get the images into the correct format and size for the model.\n",
        "\n",
        "test_transforms: These transformations are for the test set, which is used for a final evaluation of the trained model. Like the validation set, data augmentation is not applied here."
      ],
      "metadata": {
        "id": "krDuZOqKehdN"
      },
      "id": "krDuZOqKehdN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "577c0616",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-08T14:59:43.064425Z",
          "iopub.status.busy": "2023-11-08T14:59:43.064061Z",
          "iopub.status.idle": "2023-11-08T14:59:43.133864Z",
          "shell.execute_reply": "2023-11-08T14:59:43.133072Z"
        },
        "papermill": {
          "duration": 0.089025,
          "end_time": "2023-11-08T14:59:43.136356",
          "exception": false,
          "start_time": "2023-11-08T14:59:43.047331",
          "status": "completed"
        },
        "tags": [],
        "id": "577c0616"
      },
      "outputs": [],
      "source": [
        "train_transforms = v2.Compose([\n",
        " v2.Resize(256),\n",
        "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.RandomVerticalFlip(p=0.5),\n",
        "    # v2.RandomRotation(degrees=(-20, 20)),\n",
        "    v2.RandomAffine(degrees=(-10, 10), translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    v2.RandomErasing(p=0.5, scale=(0.1,0.15)),\n",
        "    v2.PILToTensor(),\n",
        "    v2.ToDtype(torch.float32),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "eval_transforms = v2.Compose([\n",
        "    v2.Resize((224,224)), # Resize to 224x224 (no cropping for evaluation)\n",
        "    v2.PILToTensor(),\n",
        "    v2.ToDtype(torch.float32),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transforms = v2.Compose([\n",
        "    v2.Resize((224,224)), # Resize to 224x224 (no cropping for testing)\n",
        "    v2.PILToTensor(),\n",
        "    v2.ToDtype(torch.float32),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Custom Dataset**\n",
        "\n",
        "This code defines a custom dataset class called MyDataset which inherits from PyTorch's Dataset class [1]\n"
      ],
      "metadata": {
        "id": "TTH_iZI7eMrX"
      },
      "id": "TTH_iZI7eMrX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d4eca4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-08T14:59:43.169223Z",
          "iopub.status.busy": "2023-11-08T14:59:43.168593Z",
          "iopub.status.idle": "2023-11-08T14:59:43.175196Z",
          "shell.execute_reply": "2023-11-08T14:59:43.174301Z"
        },
        "papermill": {
          "duration": 0.025191,
          "end_time": "2023-11-08T14:59:43.177148",
          "exception": false,
          "start_time": "2023-11-08T14:59:43.151957",
          "status": "completed"
        },
        "tags": [],
        "id": "97d4eca4"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe, transforms_):\n",
        "        self.df = dataframe\n",
        "        # We'll use transforms for data augmentation and converting PIL images to torch tensors.\n",
        "        self.transforms_ = transforms_\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.df.iloc[index]['path']\n",
        "        # img = Image.open(image_path).convert(\"LA\")\n",
        "        img = Image.open(image_path).convert(\"RGB\")\n",
        "        # img = Image.open(image_path)\n",
        "        transformed_img = self.transforms_(img)\n",
        "        class_id = self.df.iloc[index]['class_id']\n",
        "        return transformed_img, class_id"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Dataset and Dataloader**"
      ],
      "metadata": {
        "id": "w4atqaEGeBQU"
      },
      "id": "w4atqaEGeBQU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use train_test_split twice. First, to split into training and a combined validation/test set, and then again to split the combined validation/test set into separate validation and test sets."
      ],
      "metadata": {
        "id": "hCma-66PeqEO"
      },
      "id": "hCma-66PeqEO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c5990a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-08T14:59:43.208952Z",
          "iopub.status.busy": "2023-11-08T14:59:43.208657Z",
          "iopub.status.idle": "2023-11-08T14:59:43.219294Z",
          "shell.execute_reply": "2023-11-08T14:59:43.218553Z"
        },
        "papermill": {
          "duration": 0.02875,
          "end_time": "2023-11-08T14:59:43.221158",
          "exception": false,
          "start_time": "2023-11-08T14:59:43.192408",
          "status": "completed"
        },
        "tags": [],
        "id": "40c5990a"
      },
      "outputs": [],
      "source": [
        "## Create dataset and dataloader\n",
        "\n",
        "# Assuming 'df' dataframe is already created and contains 'label' column for stratification\n",
        "\n",
        "# First split: 80% for training, 20% for combined validation and testing\n",
        "train_df, val_test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=9898,\n",
        "    stratify=df['label'] # Stratify based on the 'label' column\n",
        ")\n",
        "\n",
        "# Second split: Split the 20% (val_test_df) into 10% for validation and 10% for testing\n",
        "# Since val_test_df is 20% of the original data, splitting it 50/50 will give 10% each\n",
        "val_df, test_df = train_test_split(\n",
        "    val_test_df,\n",
        "    test_size=0.5, # 0.5 of the 20% is 10% of the original data\n",
        "    random_state=9898,\n",
        "    stratify=val_test_df['label'] # Stratify this split as well\n",
        ")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
        "# num_workers can be adjusted based on your system\n",
        "num_workers = 2 if device=='cuda' else 2 # Example workers setting\n",
        "\n",
        "# Create datasets for each split\n",
        "train_dataset = MyDataset(train_df, train_transforms)\n",
        "val_dataset = MyDataset(val_df, eval_transforms) # Using eval_transforms for validation\n",
        "test_dataset = MyDataset(test_df, test_transforms) # Using test_transforms for testing\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create DataLoaders for each dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers) # Test loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49220dba",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-08T14:59:43.253008Z",
          "iopub.status.busy": "2023-11-08T14:59:43.252689Z",
          "iopub.status.idle": "2023-11-08T14:59:43.257920Z",
          "shell.execute_reply": "2023-11-08T14:59:43.256815Z"
        },
        "papermill": {
          "duration": 0.02343,
          "end_time": "2023-11-08T14:59:43.259863",
          "exception": false,
          "start_time": "2023-11-08T14:59:43.236433",
          "status": "completed"
        },
        "tags": [],
        "id": "49220dba"
      },
      "outputs": [],
      "source": [
        "print(f'train data:{len(train_df)}')\n",
        "print(f'val data:{len(val_df)}')\n",
        "print(f'test data:{len(test_df)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8768d799",
      "metadata": {
        "papermill": {
          "duration": 0.015112,
          "end_time": "2023-11-08T14:59:43.290486",
          "exception": false,
          "start_time": "2023-11-08T14:59:43.275374",
          "status": "completed"
        },
        "tags": [],
        "id": "8768d799"
      },
      "source": [
        "## Create model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the custom CNN"
      ],
      "metadata": {
        "id": "eHBny0Mg5A4G"
      },
      "id": "eHBny0Mg5A4G"
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        # The fully connected layer will be defined after we determine the input size\n",
        "        self.fc = None # Initialize as None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = x.view(x.size(0), -1) # Flatten the tensor\n",
        "        # The fully connected layer is applied here\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "L8ff8sx148tO"
      },
      "id": "L8ff8sx148tO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the Model"
      ],
      "metadata": {
        "id": "YMiFYLFsgkK4"
      },
      "id": "YMiFYLFsgkK4"
    },
    {
      "cell_type": "code",
      "source": [
        "class_size = 5\n",
        "model = SimpleCNN(num_classes=class_size)\n",
        "\n",
        "# Determine the input size for the fully connected layer\n",
        "# We will pass a dummy tensor through the convolutional and pooling layers\n",
        "# to see the output shape before the flattening step.\n",
        "dummy_input = torch.randn((16, 3, 224, 224)) # Use the expected input size after transforms\n",
        "\n",
        "# Pass the dummy input through the conv and pooling layers\n",
        "with torch.no_grad(): # We don't need to calculate gradients for this\n",
        "    x = model.conv1(dummy_input)\n",
        "    x = model.relu(x)\n",
        "    x = model.maxpool(x)\n",
        "    x = model.conv2(x)\n",
        "    x = model.relu(x)\n",
        "    x = model.maxpool(x)\n",
        "    flattened_size = x.view(x.size(0), -1).size(1) # Get the size after flattening\n",
        "\n"
      ],
      "metadata": {
        "id": "0IwqFVsd7AEe"
      },
      "id": "0IwqFVsd7AEe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "lhk_FYdIh6iL"
      },
      "id": "lhk_FYdIh6iL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Flattened size: {flattened_size}')"
      ],
      "metadata": {
        "id": "FKat4C3QheL7"
      },
      "id": "FKat4C3QheL7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now define the fully connected layer with the determined input size\n",
        "model.fc = nn.Linear(flattened_size, class_size)\n",
        "model"
      ],
      "metadata": {
        "id": "DJs4cuHtBz-7"
      },
      "id": "DJs4cuHtBz-7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "00720982",
      "metadata": {
        "papermill": {
          "duration": 0.017516,
          "end_time": "2023-11-08T14:59:47.521468",
          "exception": false,
          "start_time": "2023-11-08T14:59:47.503952",
          "status": "completed"
        },
        "tags": [],
        "id": "00720982"
      },
      "source": [
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27ec6521",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-08T14:59:47.558849Z",
          "iopub.status.busy": "2023-11-08T14:59:47.558479Z",
          "iopub.status.idle": "2023-11-08T14:59:47.569960Z",
          "shell.execute_reply": "2023-11-08T14:59:47.569141Z"
        },
        "papermill": {
          "duration": 0.032757,
          "end_time": "2023-11-08T14:59:47.572298",
          "exception": false,
          "start_time": "2023-11-08T14:59:47.539541",
          "status": "completed"
        },
        "tags": [],
        "id": "27ec6521"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer, lr_scheduler):\n",
        "    size = len(dataloader.dataset) # number of samples\n",
        "    num_batches = len(dataloader) # batches per epoch\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_correct = 0\n",
        "    for (data_,target_) in dataloader:\n",
        "        target_ = target_.type(torch.LongTensor)\n",
        "        data_, target_ = data_.to(device), target_.to(device)\n",
        "\n",
        "        # First we'll clean the cache of optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward propagation\n",
        "        outputs = model(data_)\n",
        "\n",
        "        # Computing loss\n",
        "        loss = criterion(outputs,target_)\n",
        "\n",
        "        # Backward propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimizing model\n",
        "        optimizer.step()\n",
        "\n",
        "        # Computing statistics.\n",
        "        epoch_loss = epoch_loss + loss.item()\n",
        "        _,pred = torch.max(outputs,dim=1)\n",
        "        epoch_correct = epoch_correct + torch.sum(pred == target_).item()\n",
        "    lr_scheduler.step()\n",
        "    return epoch_correct/size, epoch_loss/num_batches\n",
        "\n",
        "\n",
        "def eval(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset) # number of samples\n",
        "    num_batches = len(dataloader) # batches per epoch\n",
        "    epoch_loss = 0.0\n",
        "    epoch_correct = 0\n",
        "    with torch.no_grad():\n",
        "        # This will disable backward propagation\n",
        "        model.eval()\n",
        "        for (data_,target_) in dataloader:\n",
        "            target_ = target_.type(torch.LongTensor)\n",
        "            data_, target_ = data_.to(device), target_.to(device)\n",
        "\n",
        "            # Forward propagation\n",
        "            outputs = model(data_)\n",
        "\n",
        "            # Computing loss\n",
        "            loss = criterion(outputs,target_)\n",
        "            # Computing statistics.\n",
        "            epoch_loss = epoch_loss + loss.item()\n",
        "            _,pred = torch.max(outputs,dim=1)\n",
        "            epoch_correct = epoch_correct + torch.sum(pred == target_).item()\n",
        "    return  epoch_correct/size, epoch_loss/num_batches\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "      size = len(dataloader.dataset)\n",
        "      num_batches = len(dataloader)\n",
        "      model.eval() # Set the model to evaluation mode\n",
        "      test_loss = 0.0\n",
        "      correct = 0\n",
        "      all_labels = []\n",
        "      all_predictions = []\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            target = target.type(torch.LongTensor)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(data)\n",
        "            loss = loss_fn(outputs, target)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "            # Store labels and predictions for metrics calculation\n",
        "            all_labels.extend(target.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "      test_loss /= num_batches\n",
        "      accuracy = correct / size\n",
        "\n",
        "      # Calculate additional metrics\n",
        "      from sklearn.metrics import f1_score, recall_score, confusion_matrix\n",
        "\n",
        "      f1 = f1_score(all_labels, all_predictions, average='weighted') # Use weighted average for imbalance\n",
        "      recall = recall_score(all_labels, all_predictions, average='weighted') # Use weighted average for imbalance\n",
        "      cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "      return accuracy, test_loss, f1, recall, cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c729316",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-08T14:59:47.612112Z",
          "iopub.status.busy": "2023-11-08T14:59:47.611735Z",
          "iopub.status.idle": "2023-11-08T15:52:02.694111Z",
          "shell.execute_reply": "2023-11-08T15:52:02.693153Z"
        },
        "papermill": {
          "duration": 3135.12497,
          "end_time": "2023-11-08T15:52:02.717740",
          "exception": false,
          "start_time": "2023-11-08T14:59:47.592770",
          "status": "completed"
        },
        "tags": [],
        "id": "5c729316"
      },
      "outputs": [],
      "source": [
        "model.to(device)\n",
        "EPOCHS = 50\n",
        "\n",
        "logs = {\n",
        "    'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []\n",
        "}\n",
        "\n",
        "#Initialize the loss function and apply the class weights:\n",
        "#Pass the class_weights_tensor to the weight argument of the torch.nn.CrossEntropyLoss function.\n",
        "\n",
        "criterion  = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "\n",
        "# Optimizer which will use gradients to train model.\n",
        "learning_rate = 0.0001\n",
        "momentum = 0.9\n",
        "weight_decay = 0.1\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=weight_decay, amsgrad=False)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0007)\n",
        "lr_milestones = [7, 14, 21, 28, 35]\n",
        "multi_step_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=lr_milestones, gamma=0.1)\n",
        "\n",
        "# Earlystopping\n",
        "patience = 5\n",
        "counter = 0\n",
        "best_loss = np.inf\n",
        "\n",
        "# Start the main MLflow run that covers the entire training and evaluation process\n",
        "with mlflow.start_run():\n",
        "    # Log parameters at the beginning of the run\n",
        "    mlflow.log_params({\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"optimizer\": type(optimizer).__name__,\n",
        "        \"loss_function\": type(criterion).__name__,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"device\": str(device),\n",
        "        \"lr_milestones\": lr_milestones,\n",
        "        \"lr_gamma\": 0.1,\n",
        "        \"early_stopping_patience\": patience\n",
        "        # Add any other relevant parameters you've defined\n",
        "    })\n",
        "\n",
        "    # Log model architecture as an artifact\n",
        "    model_summary_file = \"model_summary.txt\"\n",
        "    with open(model_summary_file, \"w\") as f:\n",
        "        f.write(str(torchsummary.summary(model, input_size=(3, 224, 224), device=str(device))))\n",
        "    mlflow.log_artifact(model_summary_file)\n",
        "    os.remove(model_summary_file) # Clean up the temporary file\n",
        "\n",
        "\n",
        "    for epoch in tqdm(range(EPOCHS)):\n",
        "        train_acc, train_loss,  = train(train_loader, model, criterion, optimizer, multi_step_lr_scheduler)\n",
        "        val_acc, val_loss = eval(val_loader, model, criterion)\n",
        "        print(f'EPOCH: {epoch} \\\n",
        "        train_loss: {train_loss:.4f}, train_acc: {train_acc:.3f} \\\n",
        "        val_loss: {val_loss:.4f}, val_acc: {val_acc:.3f} \\\n",
        "        Learning Rate: {optimizer.param_groups[0][\"lr\"]}')\n",
        "\n",
        "        logs['train_loss'].append(train_loss)\n",
        "        logs['train_acc'].append(train_acc)\n",
        "        logs['val_loss'].append(val_loss)\n",
        "        logs['val_acc'].append(val_acc)\n",
        "\n",
        "        # Log metrics for each epoch within the same run\n",
        "        mlflow.log_metrics({\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_accuracy\": train_acc,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_accuracy\": val_acc,\n",
        "                \"learning_rate\": optimizer.param_groups[0][\"lr\"]\n",
        "            }, step=epoch)\n",
        "\n",
        "        # Save model checkpoints\n",
        "        torch.save(model.state_dict(), \"last.pth\")\n",
        "\n",
        "        # Log checkpoint every few epochs (e.g., every 10 epochs) as an artifact (optional)\n",
        "        if (epoch + 1) % 1 == 0:\n",
        "            checkpoint_path = f\"checkpoint_epoch_{epoch+1}.pth\"\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            mlflow.log_artifact(checkpoint_path, artifact_path=\"checkpoints\")\n",
        "            os.remove(checkpoint_path) # Clean up the temporary file\n",
        "\n",
        "\n",
        "        # chcek improvement\n",
        "        if val_loss < best_loss:\n",
        "            counter = 0\n",
        "            best_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"best.pth\")\n",
        "        else:\n",
        "            counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Earlystop!\")\n",
        "            break\n",
        "\n",
        "    # --- Test Evaluation and Logging (inside the main MLflow run) ---\n",
        "    print(\"\\n--- Evaluating on Test Set ---\")\n",
        "\n",
        "    # Load the best model state dictionary\n",
        "    model.load_state_dict(torch.load(\"best.pth\"))\n",
        "    model.to(device) # Ensure the model is on the correct device\n",
        "\n",
        "    test_accuracy, test_loss, test_f1, test_recall, test_confusion_matrix = test(test_loader, model, criterion)\n",
        "\n",
        "    print(\"\\n--- Test Set Results ---\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "    print(f\"Test Recall: {test_recall:.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(test_confusion_matrix)\n",
        "\n",
        "    # Log test metrics to the same MLflow run\n",
        "    mlflow.log_metric(\"test_loss\", test_loss)\n",
        "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
        "    mlflow.log_metric(\"test_f1_score\", test_f1)\n",
        "    mlflow.log_metric(\"test_recall\", test_recall)\n",
        "    # Log confusion matrix as an artifact (requires saving it to a file)\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(test_confusion_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    confusion_matrix_path = \"confusion_matrix.png\"\n",
        "    plt.savefig(confusion_matrix_path)\n",
        "    mlflow.log_artifact(confusion_matrix_path)\n",
        "    os.remove(confusion_matrix_path) # Clean up the temporary file\n",
        "    plt.close() # Close the plot\n",
        "\n",
        "    # The MLflow run automatically ends when exiting this 'with' block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f991a78",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-08T15:52:02.763986Z",
          "iopub.status.busy": "2023-11-08T15:52:02.763205Z",
          "iopub.status.idle": "2023-11-08T15:52:03.345650Z",
          "shell.execute_reply": "2023-11-08T15:52:03.344613Z"
        },
        "papermill": {
          "duration": 0.608465,
          "end_time": "2023-11-08T15:52:03.347969",
          "exception": false,
          "start_time": "2023-11-08T15:52:02.739504",
          "status": "completed"
        },
        "tags": [],
        "id": "4f991a78"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(logs['train_loss'],label='Train_Loss')\n",
        "plt.plot(logs['val_loss'],label='Validation_Loss')\n",
        "plt.title('Train_Loss & Validation_Loss',fontsize=20)\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(logs['train_acc'],label='Train_Accuracy')\n",
        "plt.plot(logs['val_acc'],label='Validation_Accuracy')\n",
        "plt.title('Train_Accuracy & Validation_Accuracy',fontsize=20)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluate on the Test Set and Output Metrics\n",
        "\n",
        "# Load the best model state dictionary\n",
        "model.load_state_dict(torch.load(\"best.pth\"))\n",
        "model.to(device) # Ensure the model is on the correct device\n",
        "\n",
        "test_accuracy, test_loss, test_f1, test_recall, test_confusion_matrix = test(test_loader, model, criterion)\n",
        "\n",
        "print(\"\\n--- Test Set Results ---\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(test_confusion_matrix)\n",
        "\n",
        "# Optionally, log test metrics to MLflow\n",
        "with mlflow.start_run():\n",
        "    mlflow.log_metric(\"test_loss\", test_loss)\n",
        "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
        "    mlflow.log_metric(\"test_f1_score\", test_f1)\n",
        "    mlflow.log_metric(\"test_recall\", test_recall)\n",
        "    # You might want to log the confusion matrix as an artifact\n",
        "    # For simplicity, we'll just print it here. Logging it requires\n",
        "    # saving it to a file or image."
      ],
      "metadata": {
        "id": "tcfEwW9pVOG_"
      },
      "id": "tcfEwW9pVOG_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3174.788318,
      "end_time": "2023-11-08T15:52:05.302404",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-11-08T14:59:10.514086",
      "version": "2.4.0"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}